The Epistemology of Falsification
Hypothesis Testing on the Lalonde Dataset
The guiding question is not "What is the treatment effect?" â€” it is "Can the Null Hypothesis of zero effect be credibly rejected, and under what conditions does that rejection hold?" This project operationalizes a shift from Estimation to Falsification.

Prompt 1 â€” README.md Â· Lab 8 Folder
ğŸ¯
Objective
Project Title: Hypothesis Testing & Causal Evidence Architecture
Using the Lalonde (1986) dataset, I adjudicate between competing narratives of causality to determine whether a job training intervention produced a measurable effect on real earnings. The analytical philosophy pivots from Estimation (reporting a number) to Falsification (stress-testing a claim against a principled null model).
âš™ï¸
Methodology
Parametric â€” Welch's T-Test
Estimated Average Treatment Effect (ATE) on 1978 real earnings
Welch's formulation handles unequal variances between groups
Produced t-statistic and p-value under normal sampling assumption
Type I error controlled at explicit Î± threshold
Non-Parametric â€” Permutation Test
10,000 resamples to build empirical null distribution
Robust to right-skewed, zero-inflated earnings data
Validates parametric result without distributional assumptions
Convergence of both methods strengthens inference
ğŸ“Š
Key Findings
+$1,795
Statistically significant lift in real earnings for job training participants. Null Hypothesis rejected via Proof by Statistical Contradiction â€” results consistent across both parametric and non-parametric frameworks, ruling out distributional artifacts as a confounding explanation.
ğŸ› ï¸
Technical Approach
Dual-method validation deployed in parallel to cross-validate conclusions and guard against model misspecification. Significance threshold explicitly set and justified â€” acknowledging that Î± is a decision boundary calibrated to the cost of false positives, not a universal scientific default.

scipy.stats â€” T-Test
numpy â€” Permutation Resampling
Random Seed Fixed â€” Reproducible
Type I Error Control
Welch's Formulation
ğŸ’¡
Business Insight: Hypothesis Testing as the Safety Valve
In the modern data economy, calculating a statistic is a commodity. The scarce skill is Causal Evidence Architecture â€” designing tests that distinguish genuine signal from noise. Without rigorous hypothesis testing, organizations fall prey to:

ğŸ²
Data Grubbing
Iterating over metrics until significance appears by chance, inflating false discovery rates.

ğŸ”€
Spurious Correlations
Optimizing on patterns that don't generalize, leading to flawed product decisions at scale.

ğŸª„
Survivorship Bias
Drawing conclusions from visible outcomes while ignoring the data-generating process.

Companies like Netflix and Uber don't ask "Is the p-value below 0.05?" â€” they ask "Is the experimental design valid?" and "Are we controlling for the right confounders?" This project treats hypothesis testing not as a checklist item, but as a framework for principled belief revision under uncertainty.
Prompt 2 â€” Concept Extension Â· Portfolio README
ğŸ§ 
Return-Aware Experimentation â€” Concept Extension
3â€“4 sentences Â· Portfolio-ready
Return-Aware Experimentation is Netflix's framework for calibrating decision thresholds to the asymmetric cost of errors, rather than applying a uniform p < 0.05 standard borrowed from academic convention. Where academic statistics treats Î± = 0.05 as a near-universal norm, Netflix recognizes that the cost of a false positive (shipping a harmful feature to 200M users) is categorically different from the cost of a false negative (delaying a beneficial one). This means decision thresholds are business parameters â€” tuned to the revenue, risk, and reversibility of each specific decision â€” not immutable scientific laws. I apply this principle by treating my significance thresholds as explicit choices I can defend, not defaults I inherit.
